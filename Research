Introduction
Time: What is it? What false assumptions do we hold about it? And what could we achieve given the right ones? These are questions this text will grapple with. Of necessity, the resulting models will offer partial answers at best, limited by the state-of-the-art science as we currently understand it. There is little disagreement that modern science’s model of time is, at this juncture, incomplete. 
Yet our models may possess more of the necessary parts than we assume. Surprising experimental results and theories exist in niche fields where time is an unavoidable part of investigations. What has generally been lacking are attempts to construct better models of time and put them into plain language. Given the rarity of scientists in relevant fields who write for a popular audience, this reluctance may stem more from hesitancy to attempt to build plain-language models than satisfaction with contemporary models of time. Compounding this lack of interest in relating what happens in the lab to our everyday experience may be the fact that time’s behaviour is mostly studied in the field of relativity, while some of the most interesting experimental results come from quantum physics, where time is a priori assumed to be universal and absolute [1]. 
This text shall survey several of the surprising results experiments involving time have produced, and attempt to formulate an intuitively satisfying theory of time compatible with these results. As such, it will make both scientific assertions and propose how to interpret them. Though the observations and predictions require detailed explanation, the basic assumptions underpinning this model are familiar and simple enough that they can be summarised as follows: 
Key to this investigation is the concept of holons. Introduced by philosopher Arthur Koestler [2], a holon is any level of organization that cannot be usefully understood by understanding its components. It is extremely difficult, for example, to predict the behaviour of a cell by modelling each of the molecules comprising it, even though useful predictions can be made when the behaviour of the cell as a whole is modelled. The cell is a holon: it cannot be modeled as a group of subcomponents without information being lost in the model. Holons of some kind exist at every scale humans have looked for them. 
Entangled particles may be the most concrete example of holons – their entanglement means that we cannot treat them as discrete particles without losing information. However, while the need for a concept like holons can be proven at such a scale, the concept is more often used intuitively at larger ones. Anywhere we use integers – be it for quarks or galaxies – we are using the holonic model. 
Another important interpretation that underpins this text is the Qbist interpretation of quantum mechanics. This interpretation describes the observations referred to in quantum mechanics as subjective and only relevant to the observer making them, and that to a third party not observing any part of the measurement process the wave function of a quantum system remains indeterminate until some data from the initial interaction reaches it [25].
Taken together, these concepts of holons and the QBist interpretation of quantum mechanics inform a third interpretation which this text proposes: that everyday, macroscopic phenomena are large scale systems operating on the same rules as quantum systems, and that the same concepts and intuitions are relevant at both scales. This is not necessarily an assertion about behaviour. A baseball thrown across a field and the moon’s orbit around the Earth can both be described by Newtonian mechanics, but it would be unreasonable to think we could predict the moon’s effect on the tides by observing the behaviour of the baseball. However, different rates of information transfer in a universe described by QBism lends physical validity to the everyday experience of holons. This observation of the rate of information transfer ties into some very interesting theories about the nature of observation and experience such as Tononi’s or Penrose & Hameroff’s theories on subjective experience – more on this later. 
This text therefore proposes the reader consider holons as a serious concept, important to everyday experience in a physical sense, that they stay open to the ‘subjective observations’ interpretations of quantum mechanics, and that taken together these interpretations allow an intuitively satisfying understanding of how the laws of quantum mechanics express themselves at macroscopic scales. The fourth, and most challenging interpretation this text will present is that holons, which we traditionally think of as three dimensional, make much more sense when they are assumed to be four dimensional. This is not a wholly novel idea - Hadley proposed systems of four-dimensional geons as a means of reconciling quantum mechanics and General Relativity [31]. While there is interest in four-dimensional models of quantum mechanics from the physics community as a potential route for reconciling Quantum and Relativistic models, this text is not intended to sort out these cosmic questions at the level of fundamental mathematics. Rather, it aims to present a conceptually satisfying theory of time which borrows concepts from quantum mechanics but is relevant to macroscopic holons like people or mental processes. This theory of temporal holonics will be successful if it predicts the otherwise unexpected observations we encounter in both experiment and everyday life, both around time itself and adjacent areas such as causality and perception. Tackling such phenomena necessarily raises interesting questions, which this text will attempt to explore. To begin this exploration, we will consider two very strange experimental phenomena that will be useful to keep in mind when thinking with time.
Prior Research
Two particularly confounding experimental results are known as the quantum Zeno effect and the quantum eraser. Though some of the theories and experiments this text draws on are controversial, both of these phenomena are not just highly strange but also widely accepted in the scientific community. 
The quantum Zeno effect is the observation that it is possible to arrest the evolution of a quantum system by measuring it repeatedly in quick succession [3]. We might say this observation confirms that a watched (quantum) pot never boils – certain systems seem to need privacy to change their state and could remain in their initial conditions indefinitely if they are observed continuously. This privacy necessary for change to take place meets the formal definition of energy, though not of a form we typically think of. This text will refer to the energy required to overcome the quantum Zeno effect as transition energy, and while it will be relevant to some of the time-adjacent topics this text will touch on, it is not immediately obvious to our classical-physics based intuitions how not being observed could be a source of energy.  One interpretation is that in order to change its state, a quantum system beginning in state S1 must enter into a superposition with a state S2 before there is a nonzero chance of observing it in S2. If the system is repeatedly observed in state S1 at an interval taking less time than it would take to enter into superposition of S1 and S2, the odds of observing the system in S2 tend towards zero. This minimum time to change state is a holon – its behaviour cannot be predicted by analyzing the behaviour of smaller ‘slices’ of time, and a quantum system cannot change its state without going through the interstitial uncertainty that a holon of time unbroken by observation brings. This image of a system with known initial and final states that requires an uncertain interstitial period in order for a change in state to occur will be important throughout this book. Note that the times of measurement at either end of the period of change can be known to an arbitrary degree of certainty, but the interstitial period of transition cannot. Unlike a moment of interaction which can be placed at a point in a timeline to any desired degree of precision, this interstitial period of change has a minimum length, and attempting to further subdivide it alters the behaviour of the system.
Another phenomenon that challenges the way we think about time from our classical assumptions is that of the quantum eraser. The quantum eraser is a variation of the classic ‘double slit’ experiment [4]. In this version, a photon is split into two, lower-energy photons on its way to the standard double-slit wall. One photon continues as in the standard experiment, to pass through the wall and either leave or not leave an interference pattern on the far side. The other continues to a reading apparatus further from the point at which the photon split than the far wall and, to the degree it can be said of a photon, spends more time in transit. 
The results are decidedly counterintuitive. Because the photons are entangled, measuring the behaviour of the one that travelled further gives information about the other. This is almost expected – we are used to situations where knowing part of a system lets us know about other parts with which it was entangled. However, whether or not the second photon that travelled further is observed or not directly impacts whether or not the first photon shows an interference pattern, even though the first photon has already impacted the photosensitive screen on the far side of the two slits by the time the second photon has either been observed or not observed.   
Our intuitive sense of time, informed by classical physics, rebels at this result – we do not think of causes as having effects before they happen. If we wish to retain this axiom – and this text will not attempt to refute it, if only for semantic reasons – then we cannot think of one event happening ‘before’ the other. We cannot ‘cut up’ the chain of events into individual links – both photons having their respective interactions are part of a whole that cannot be further subdivided along spatial or temporal lines, they are a holon that spans not just their respective locations in space but the span of time between the instant they are split and the instant one is measured.
The concepts of quantum erasers and the quantum Zeno effect demonstrate that time is not, as we think of it, infinitely divisible. It is divisible, certainly – interactions can usefully be thought to happen at precise instances – but there are also stretches of time that are more accurately described as being extant. Systems that are extant in time display certain behaviours only if they are allowed to exist on a continuum that stretches across multiple points in time, and the interstitial points exist not as points but fields, existing in no particular point in spacetime. 
Experiments such as these seem to directly contradict our existing theories of time. This is certainly the case for most mainstream, intuitively adopted models of time. There are, however, several theories that, while not used to inform mainstream intuitive ideas about time, seem to better fit the data. Though this text is focused on better intuitive models of time, it is valuable to study these theories in the more narrow sense their authors presented them in before seeking to relate them to everyday experience.
One such theory put forward by Oeckl is that for a theory of quantum gravity to give rise to Einstein’s theory of special relativity, it is necessary to posit a ‘general boundary’ to one’s models rather than presupposing an infinite spacetime [6]. Key to his approach was the idea that for such a bounded region, as he puts it, “Transition amplitudes are associated with regions of space-time and states are associated with their boundaries.” Here we see the beginnings of a hard-data model in which definitive, three-dimensional states exist only at the boundaries of 4-dimensional regions of spacetime, with the 4-dimensional interior being represented by a linear combination of states. Though Oeckl proposes these boundaries to ensure that quantum physics display the same ‘locality’ of results as relativity, the concept is similar to the ‘temporal holon’ theory proposed here. 
Penrose and Hameroff’s Orchestrated Objective Reduction theory similarly posits bounded spacetime. Their attempt to address the hard problem of consciousness posits that a given superposition has its own spacetime curvature, with each instant of consciousness happening when more than one of these curvatures interact [7]. Similar to Oeckl’s theory, this model posits a collection of bounded regions whose internal states are quantum but whose interactions are classical.
Tononi arrived at a similar model through studying consciousness and neural structures [8]. This model is mathematically rigorous, relating subjective experience inside a given boundary to both the amount of information generated and the degree to which the information integrates. Tononi defines the first as the scale at which uncertainty is reduced, using the example of a dice which has six possible states while tumbling, which are reduced to one when it lands. This image of uncertainty being reduced relates very closely to descriptions by theorists like Oeckl and Penrose of a system being in superposition across a four-dimentional continuum before ‘collapsing’ into a three-dimensional state. How Tononi describes the degree of integration of information is harder to put into lay terms, but we might say that the degree of integration of information relates to the maximum information that can be passed across the least sensitive link when modelling the system as a network. This description allows the model to determine the proper boundaries of a conscious system, because if it is embedded in a system that can pass more information across a less sensitive link, the ‘experience’ of the larger system is indistinguishable from the subcomponent. Tononi’s research focuses on multiple subregions of the brain, but with a bit of speculation it also predicts an embedded model of consciousness: If a team of people is able to integrate a lot of information between the members, then it meets the definition of being conscious – however, the least sensitive links will typically be between the team members. Within the individual team members’ brains, the least sensitive connections will typically still have more ‘bandwidth’ than the members can exchange between each other through practical communication. If the ‘bandwidth’ of the communication between members were to exceed that of the members’ internal divisions, their consciousness would no longer be functionally separate from one another. Or at least, the separation would no longer be where we would intuitively draw it – if a team’s major divisions in communication are e.g. along ideological lines, the cleanest place to subdivide it might be, not at the boundaries of the team members’ bodies, but inside the brain of the member most undecided between the ideologies driving this internal division! 
Tononi’s work, it must be stated, deals with brain regions at the scale of neural columns and individual neurons – being mathematically rigorous, it is not directly applicable to speculation about dynamics at the scale presented here. However it is relevant because, by formalising the meaning of individual experience, it can help us think more precisely about what a holon in our context is. A holon in the philosophical sense is a fairly loose concept, able to be conceptualised fairly arbitrarily. Tononi’s work, by associating individual consciousness with the most information passed over the weakest internal link, offers the prospect of an objective boundary for an individual consciousness, both for instantaneous experience and in the realm of memory. This quality of subjective consciousness will be important later in our exploration.
Entanglement is common defined as a group of particles where the quantum state of each particle cannot be defined independently. This already brings to mind the definition of a Holon as an individual unit. Consider Tononi’s definition of a complex in light of concepts like entanglement and relaxation time. By his definition, it would be impossible to know the interior state of a complex by observing its behaviour. Because more information is passed internally (even among the narrowest ‘choke point’ between internal subdivisions) than can be expressed externally, it is impossible to definitively deduce internal state via observing behaviour. The possibility exists of a partial superposition of the entire complex from the perspective of an outside observer. Of course, an internal means of recording aspects of its state would practically guarantee that they would become available to the environment eventually. And memory is a very useful mechanism. But having a means of suspending such internal measurement would allow the organism to make use of quantum effects in a neural context, as some organisms are already known to do e.g. during photosynthesis [9].
A Theory of Holonic Temporality
The clash between quantum and classical physics has the potential for so much cognitive dissonance that we tend to find various means and shortcuts for reconciling them, even at the cost of precision in our models. One such questionable mean is to naively separate types of systems by scale. In this approach, systems at very low temperatures and scales are modeled with quantum mechanics, while everything larger and warmer is modeled as a classical system [10]. This separation of responsibility works well for making predictions, but the models  remain stubbornly irreconcilable.
Central to the themes presented in this work is the idea that quantum mechanics are present at every level of organization and can be recognized as a factor of some degree of relevance in any investigation. When two particles are entangled, we might say that they are properly modeled as a single system and that their respective states cannot be described independently of the other. Although the two particles can each be thought of as separate, their behaviour can only be understood when we take them both into account. This concept of entanglement from physics corresponds directly with the philosophical concept of holons. A holon can only be thought of as a single thing, any attempt to reduce it to its components causes a loss of information which will decrease the accuracy of predictions. 
Precisely when the uncertainty of quantum mechanical components will cancel each other out and when they will compound is not a straightforward question. However, we can be confident that when we cannot make direct observations at the quantum scale, they will have the potential to cancel one another out, and applying the philosophical concept of holons can indicate precisely when we cannot directly observe the interiority of a system. Any system which cannot be accurately modeled as the sum of its parts displays behaviour particular to its own level of organization. The inability to observe all components of such a system indicates the impossibility of ruling out the significance of quantum behaviour among those components.
It may be more logical to take a gradated approach to the scaling problem than this sharp segregation of quantum and classical models. Which model is applied to a given system is well known to be a question of what is most functional, and it is not always necessary to choose between alternatives as though it is a binary choice. For example, in fluid dynamics, whether a flow is turbulent is determined by a nondimensional parameter called the Reynolds number. This number is given by the equation 
R_e=ρϑL/μ
Where R_e is the Reynolds Number, ρ is the fluid’s density, ϑ is the velocity of the fluid, L is the length of the airfoil or diameter of the pipe with which it is interacting, and μ is the dynamic viscosity of the fluid. Fluid flow is generally considered laminar if the Reynolds number is below 2300 and transient or turbulent above this value. I had always thought of turbulence as an objective measure. It was quite a paradigm shift for me when, in speaking with a mentor, he casually pointed out that the reference length L is rather arbitrary, able to refer to an entire wing of a plane or the wing’s smallest adjustable flap. A single flow could therefore be both laminar and turbulent, depending entirely on the size of the object with which it is interacting. Turbulence is only a useful concept in relation to a specific solid body with which the flow is interacting, and perturbations that are turbulent to one object could be laminar to another.
Einstein’s theory of relativity has demonstrated that the relationship of the observer to what is being observed can influence how an experiment turns out [11]. Despite how widely this theory is accepted, many of us continue to habitually think in terms of a single truth, as though there is an unimpeachable observer who could, if queried, provide a consistent answer to all questions about the state of the universe. Although this premise underpins much of classical physics, we no longer accept it if we wish to form an intuitive understanding of how time really works given the theory of relativity. If there were a single unimpeachable observer, the effects that only take place in the absence of an observer could not happen. 
It is difficult, if not impossible, for an experiment to distinguish whether the collapse of a wave function is an absolute fact caused by observation or whether the distinction between superposition and collapse only has validity relative to an observer. To think of entanglement as an absolute truth is slightly simpler, but intuitive understandings of something being true only relative to an observer are not so far outside of our daily experience. Momentum, for example, is commonly understood to only be a meaningful concept given an observer with its own momentum. It may help the reader to take a similarly relativistic view of wave functions. Just as an identical fluid might appear laminar or turbulent depending on the pipe through which it flows, the best model for a system might be quantum or classical depending on the nature of our interaction with it. The question of whether a system’s nondeterministic aspects are safe to ignore for the purposes of prediction already implies the validity of different ways of observing reality depending on the specific system. A more useful approach may be to think of systems not as either entangled or classical, but as a density function similar to how we visualise quantum systems.
This can broaden . Consider a density function representing the possible states of a system: the more precise our observation of it, the more we can treat this density function as though it were a single value – but there is no time at which our measurement error changes from nonzero to zero. Whether uncertainty is relevant to our inquiry is an arbitrary choice. We must first decide how many significant digits we will work in, and only then can we determine whether the uncertainty in the system is large enough to affect it. When we work at quantum scales, the uncertainty thrown up by the probability distribution is typically larger than our least significant digit. At classical scales, it is typically smaller. But uncertainty is never eliminated as we change scale, there are merely scales where it can be safely ignored. Whether a system is quantum or classical is not a function of the system, but of what we are measuring or observing about it. 
McFadden and Al-Khalili present a very interesting take on the so-called ‘scaling problem’ in an article on adaptive mutation [12]. Drawing on Zurek’s expression of the time scale over which quantum coherence is lost [13], they presented a possible explanation of how adaptive mutation can cake place, despite the low probability a given adaptive mutation would have. Central to their model is the idea of relaxation time, or how quickly the energy of a particle dissipates due to its interaction with the environment. This variable is also important to the quantum Zeno effect. 
This concept may be important to the question of how quantum mechanics, which clearly determines the behaviour of the smallest building blocks of our world, affects the world of our everyday experience. A marble rolling off a table and the moon orbiting the earth produce radically different phenomena while obeying the same laws of gravity and inertia. In the same vein, we should expect the laws of quantum mechanics to be as present at our scale as they are at the scale of atoms, without expecting behaviour to be identical. The variability of relaxation time with scale, and the necessarily partial nature of (de)coherence at larger scales, gives us a means of visualising this transition more smoothly, as opposed to the hard transitions between quantum, classical, and relativistic behaviour we typically assume when considering different scales of physics.
We tend to think of particles as being purely in superposition, and then ‘decohering’ into a single, classical position following an interaction. However, this idea again assumes there is a single infallible observer who keeps track of the ‘true’ state of the particle. In fact, each observer has a perspective whose measurement can itself be in superposition from the perspective of another observer [14]. Consider instead a collection of particles, each with its own perspective. They are isolated and, therefore, in superpositions of various possible states. The higher the temperature of the collective system, the more likely any given pair of particles is to interact, resulting in a shorter time to decoherence. We could also say, the higher the temperature the more quickly each particle’s energy dissipates through the system as a whole. 
Consider how this would appear from the perspective of the particles, in this case a group of protons in a DNA chain. A proton is isolated and in superposition. It interacts with another proton, causing both their wave functions to collapse – or as we might prefer to conceptualise it, their interaction takes place in a particular way, thus limiting the possible future states either can display.
Now consider a third, more distant proton which has not yet interacted with either of the first two. From its perspective, both of the first two protons are entangled and in superposition. When it eventually does interact with one of them, some of the information transmitted to it will relate to the history of these two particles’ interactions. From a fourth proton’s perspective, all three would have a probability density function which would resolve into a particular position and possibly a particular history when they interact. The time required for a particle’s energy to ‘dissipate’ is essentially the time required for it to interact with its environment to such a degree that it leaves a ‘signature’ in the ambient heat. The ‘relaxation time’ of a set of coherent particles is a measure of how quickly their energy differences dissipate[12]. 
Consider this quantity in the context of the scaling function. Energy within a system behaves differently depending on scale. A system comprised of many particles is more likely to interact with a new particle, but its exact state will not be determined as quickly. To put it another way, a larger system takes longer to make a unique signature in its environment – it has a smaller ‘surface area’ relative to its internal complexity. The larger a system is, the easier it is to observe it partially and the harder it is to observe it completely. Much more data can be gathered immediately, but there is also more uncertainty behind what is initially obvious. 
If we think of scaling in Quantum Mechanics as a function of energy dissipation, we can intuit how scaling changes the nature of quantum effects. The larger the system, the more likely there is to be a partial interaction, and the less likely there is to be a complete interaction. Some internal entanglement is likely to remain in place, even as some surface level interactions reduce the size of the possible state-space. This approach therefore offers an intuitively satisfying means of scaling up “bounded region” theories of quantum mechanics as discussed by Oeckle, Hameroff, and Penrose. 
It is important to remember that this reduction in possible states happens from a perspective. An interaction between a system and an observer reduces the system’s possible states to the observer, while a third party that has not interacted with either system or observer perceives a closed system with a different probability distribution, or a different “interiority”. 
Many scientific models treat the abstract “heat bath” in which a system resides as a sort of universal observer. This can certainly be useful—it helps the math make sense, and it may be an accurate description of the universe in a fundamental sense—but we must remember that as individuals making our own observations, that perspective is not directly available to us. When we speak of “interaction,” we typically refer to two holons like an experiment and an investigator interacting with one another. Interactions between a holon and the “heat bath” in which it exists is another matter, The universe may have access to a lot of information about the experiment via the heat bath, but only a small portion of this would be accessible to the investigator. Energy diffuses from a holon to the environment as a function of surface area relative to volume. A large, complex holon such as a human will always be in a state of partial diffusion: some aspects of the holon will be observable to other holons, while other aspects will be purely “internal” over their duration with only their tertiary effects ever available to be perceived by outside observers. Holons are, in some ways, defined by their rate of exchanging information internally compared to the outside world. 
Internal interactions still take place in holons of course. In McFadden and Al-Khalili’s example, protons in a DNA molecule would still jostle their neighbors, but precisely how this happens is better described as a probability distribution than through classical modelling. 
Our classical view of time probably causes much of the trouble we have when trying to think intuitively about matters of quantum physics and scale. We are somewhat accustomed to thinking of holons that extend through space as not quite infinitely divisible. This is an aspect of what it means to be entangled: the subcomponents of an entangled system have some relation to one another, and if they were to be treated as separate and unentangled, relevant data about the system would be lost. In a quite literal sense, the entanglement of a system is synonymous with the system being a holon. 
Holons can also extend and be entangled through time, as experiments like the quantum eraser show. Our conventional way of thinking about time as infinitely divisible, much as we used to think about light, is therefore probably incorrect. Systems can no more be infinitely divisible by location in time than by location in space – that is to say, we might specify spans of arbitrarily small divisions, but some quanta will extend beyond these boundaries, and we will lose potentially important information if we try to ‘divide up’ a holon without taking into account its own boundaries. 
Working with the model
From a mathematical standpoint, this may not seem too significant an adjustment. Instead of a three dimensional Hilbert space that can be analyzed arbitrarily within those three dimensions we now work with a four-dimensional space. Calculations of ‘surface area’ as they relate to energy dissipation and its effects on decoherence, take on a different dynamic, much in the same way modelling heat diffusion on a plane is different from modelling it in a 3D space. But mathematically, a four dimensional space is not terribly different from a three dimensional one. 
In the world of physics however, things look rather different. The states of a system at two adjacent points in time are causally linked in different ways than adjacent points in space. If we consider the probability distribution of a system’s state, this distribution varies very differently across the time dimension than across the space dimensions. 
We are reasonably intuitively receptive to nonlocality in space. That a particle can have a probability distribution describing its location at any given moment – or that a larger object can have a nonlocal aspect to its state which dissipates as more energy is exchanged with its environment – is fairly easy to visualise. Extending this nonlocality to time however breaks down our intuitive image of a probability cloud with various ‘densities’ corresponding to the likelihood of interactions occurring at that place. If for no other reason, visualization becomes harder when including the time axis because the state at a particular point in time is highly dependent on its state at adjacent points. 
Consider the proton in a DNA molecule from our earlier example. With no interactions, its state is a probability density, with any interaction it has leaving an ‘imprint’ on its local environment. This ‘imprint’ narrows down its possible states to an increasingly determined, classical state. We also refer to this process as its energy diffusing into the environment. 
At the moment of each interaction however, our particle of interest collapses not only its state at that instant but reduces the possible states at adjacent points in time as well. These states are not as widely restricted as the state during an interaction, but we might think of these states as the narrowing of a stream. Any bit of water must pass through the narrowest point – the actual interaction in our metaphor – and the farther away from this narrowing in the flow, the more potential variation there is in the water’s position. 
It is fairly easy to think of a particle’s position as a point within a three dimensional probability cloud of heterogenous density. If we represented a cross-section of these likelihoods numerically, we might see something like this:
 
And this causes little trouble to our intuition, as such things go. We are comfortable envisioning a ‘haze’ of probability around a basically classical universe. But adding a time dimension makes things much more difficult to visualise properly, since this ‘haze’ suddenly becomes very path-dependent. Suppose a system’s state changes over time, diverging from the normal distribution depicted above:

                                                     
Now suppose this density distribution cloud were to encounter an outside force which reduced its state:
                                                 
The interstitial probability distribution may ‘in fact’ be bimodal, but suppose this sort of continuous history was inaccessible to us. The most reasonable assumption would usually be that the progression was never bimodal, but more as follows:
                                   
In most interpretations of quantum mechanics this probability cloud does not represent merely our best guess about the system but the actual interstitial connection between the known beginning and end states. Without an interaction, any number of interstitial probability distributions can be reasonably imagined. 
Our typical visualization of these systems involves zero-dimensional particles with three-dimensional probability clouds, interacting in unquantified and infinitely divisible time. If we think instead about chains of events, we must instead visualise one-dimensional progressions whose probability densities can swing widely depending on previous states. Behaviour can also be strongly impacted by what sort of energy diffusion leaves a partial impression on the local heat environment – in any actual chain of events, the progression between events will be partially constrained by the impression left on the heat environment, but multiple (though presumably countable) possibilities would still ‘take place’ between events. This is what a holon in time is -the undetermined stretch of nonlocality between knowable events. 
The concept of a temporal holon is not that difficult to imagine from the perspective of an outside observer. But recall that example of a proton in a DNA chain. A proton could be in a superposition of states that had and had not interacted with a neighboring particle. It is a simple matter to imagine a proton beginning in one position, going through an ambiguous period of transit, and interacting with helicase in a particular position different from the one in which it started. 
Once we are comfortable with dynamics like the Quantum Zeno effect, we can accept this period of transition to be necessarily ambiguous – we might think of the energy dissipation of a particle like a proton preventing it from ‘having’ sufficient energy to change states. As a result, the transition must be unobserved to connect the first and last states. 
But how does this period of necessary ambiguity ‘look’ from the perspective of the proton in transit? Presumably, multiple possibilities would ‘appear’ open to it, and the standard linear combination of states would apply [15]. A proton is likely a simple enough entity that it would have no ‘memory’ of taking a particular path through the transition period, aside from its momentum at the moment it interacts with the helicase. The path it took between its initial and final states will be unknowable from the perspective of the helicase. 
Suppose now that we replace the proton in our example with a human or similar agent, and ‘zoom out’ far enough that only the vaguest details, such as initial and final positions in spacetime, are knowable from the perspective of our observer. We would regard the temporal co-ordinates of the initial and final observations as ‘fixed’. However, the relaxation time, that is to say the period during which the agent is in motion and interacting only with its heat environment – is not available for event formation as seen from the observer’s standpoint [12]. 
Unlike a proton a humanlike agent would form memories of the transition time. But like all other aspects of superposition extended through time, they will be relatively fluid – eyewitness testimony is weighted quite lightly in court proceedings precisely because it does not really ‘nail down’ observed events, being rather changeable and context-dependent [16].
When the scale between agents is sufficiently massive, the meaningful effects the smaller agent can have on the larger becomes increasingly rare. This goes hand-in-hand with the smaller agent being better described as a wave than as a particle. However, entanglement in space and entanglement in time work differently as one ‘zooms out’. In space, classical mechanics seem to appear naturally through a law of averages as scales get increasingly large. In time, progressions are highly dependent on possible past states. Suppose a past state is not classically knowable. The longer an intervening holon of time lasts, the more different the boundary states can be; the more unpredictable the event progression is, and the more unpredictable the effect they have. Unlike holons in space, which become more predictable as the scale increases, holons in time become more chaotic as they scale up; from completely static at sub-zeno timescales to truly unpredictable at longer scales. 
Classical ideas about causality are insufficient to explain the interplay between observations at different times and the transitionary holons which connect them. As the quantum eraser experiment shows, an interaction can restrict the possible states described by a quantum wave function prior to the interaction taking place. However, a quantum wave function evolving in a particular way also affects the set of future interactions which are possible. Agents at different points in time can interact bidirectionally, rather than unidirectionally as we tend to imagine. 
This is confusing to try and model as modelling how a complex system will evolve within even a few time steps becomes increasingly difficult with the complexity of the system. To allow for a given interaction to affect the possible past as well as the possible future significantly increases the difficulty of understanding the dynamics.
This is one reason why the idea of holonic temporality can be so powerful – modelling a holon does not require considering actions outside itself, nor the effects of past causes or future attractors. A holon in time has a consistent logic when taken as a whole, it does not require the modelling of interactions at different points in time. For modelling purposes, it occupies only a single continuum in time which cannot be subdivided without a loss of data – past and future are therefore not meaningful concepts inside a holon extended through time, allowing us a middle step in our conceptualization. Between a naïve understanding of interaction of past-future, and the conditionally bidirectional temporality we seem to inhabit lies a means of modelling via simultaneity. 
Simultaneity as the term is used here does not necessarily mean events occurring at identical instances in time. Rather, it refers to processes that cannot be subdivided along the time dimension without a loss of data. Two particles can become entangled before being separated, thereafter remaining a single holon despite existing at separated points in space. In the same manner, a process can become entangled and remain a single holon despite existing at different points in time. 
While the unobserved freedom needed to overcome the quantum Zeno effect is probably the most literal example of such a holon, a very abstracted example may be a narrative. Although a narrative can be subdivided, meaning is lost if the subcomponents are analysed in isolation. Consider the example of Chekov’s gun [17] across a three-act play. The gun is seen in the first act, making a promise to the viewer that must be kept by the end of the third. What is the meaning of the gun in the first act? The answer is very different if we are analyzing it as separate from the third act. Chekov’s gun is a holon – if it is analysed only in the first act it lacks a payoff, analysed only in the third it lacks foreshadowing. Like entangled particles at different positions, there are logical divisions between the gun’s interactions with the narrative, but treating the different appearances as isolated reduces the information we can draw from them. 
The obvious question then is what changes with the presence of data only available when a progression is viewed as a larger holon in time. The addition of information has an obvious effect in terms of the meaning that is drawn from the thing being modeled. The nature of this is obvious if we consider the example of a narrative. If we sub-divide a play into three acts, the meaning of each act analyzed individually will be very different from the meaning drawn from the play as a whole. 
Meaning is an elusive concept. It can seem as though it is purely a metaphysical change that is introduced by the added data a complete holon brings. Consider however, that it is precisely meaning in the context of continuous time that we weigh when making decisions [18]. Adding information, though it tends to make decisions more difficult, also tends to make them better. 
An important question centers on how accessible the information which requires dealing with holons is to an agent acting during the span of time bounding the holon. It seems intuitive that an agent inside a holon has the option to interact with the other parts of the holon in such a way as to cause a collapse of the wave function. Returning to the example of a proton in a DNA chain, the proton has no internal memory – thus, while it might have several interactions from its perspective, an outside observer would only observe such an interaction if it changes the behaviour of the molecule. 
If the agent is a human engaged in some activity, the memory the human builds of the chain of events would be expected to cause a collapse of the probability-space. Humans are constantly orienting themselves in time, and part of the effect this has is to limit the length of time that could be understood as a holon – and hence, to limit the meaning that can be derived from it. This may be one reason why narrative and storytelling is such a fundamental mode of human cognition – it allows us to extend our sense of causality explicitly beyond what is possible implicitly. It lets us draw out relations and meaning by drawing back from a series of events that might seem disconnected and meaningless while they were being experienced. 
Potential Applications
It would appear that there is an additional means which humans have access to for interacting with a holon extended unusually far through time. Flow state is a quite mysterious process, with significant controversy and much of the available data coming from subjective reports. One of the few pieces of hard data recovered from this phenomenon is that it is characterised by a reduction of activity in the frontal and medial temporal lobes [19].
This is not terribly surprising to anyone recalling an experience of flow. Time dilation – as can occur during a car crash – or compression, which often occurs during artistic endeavours, are both common characteristics of flow states. However, it is counterintuitive when we consider the feats performed in flow states. Why should our ability to orient ourselves in time be suppressed during times of flow? People in flow often make and act on extremely precise predictions, stories abound of star performers ‘skating where the puck is going to be’ [20] or anticipating their opponents’ moves [21]. Why should this become easier with a suppressed ability to orient in time?
With a traditional model in which time is homogeneous and infinitely divisible we would not expect it to. The ability to imagine other times and reason how conditions at one will affect another should be key to making precise predictions and setting ourselves up for future success. Doing any of these things should be harder if we are unable to orient ourselves in time. But with a holonic model of time, this makes a certain amount of sense. If our frontal cortex typically works out how different moments relate to one another, it would be responsible for interaction between them – and therefore in a collapse of the wave function which reduces the possible outcomes. 
It is possible to affect the ‘path’ of a photon passing near a detector and thereby changing the set of possible locations it will arrive at. Not observing a photon in such a situation allows the wave function to encompass more possible states, leading to the spooky interference effects we associate with these experiments. Not observing adjacent ‘chunks’ of time could similarly allow a single wave function to encompass the potentials of both. 
Though somewhat radical, this model of time and flow offers explanation for observations that would otherwise be fairly confounding. In addition to a flow state’s paradoxical heightening of predictive ability while deactivating the frontal lobes, it also offers explanation for why flow, despite offering clear advantages, is still a transient phenomenon which no one seems able to maintain indefinitely. Traditional explanations involve the burn rate of neurotransmitters and the nutritional requirements associated with replenishing them. A time-as-holon model offers an alternative explanation – if flow requires the brain to model an entire holon, then there would be an upper bound on how complex it could be before the computational limit of the brain is reached, and it becomes impossible to represent more time without ‘chunking’ it into smaller, simpler holons interacting. If reliable flow is simply a question of nutritional requirements, we would expect to see the typical person spending much more time in flow state so long as society continues to produce excess food. If, however, flow state is limited by the ‘largest’ holon a brain can model, we can expect to see limits to what can be done in this regard for much longer.  
The tendency for flow to appear with non-judgemental modes of thought is also consistent with the theory of temporal holonics. Flow typically happens in high-stakes situations, ‘good’ and ‘bad’ outcomes are both possible and easy to contrast. However, a person in flow does not typically recognize any action they take as mistaken until after the flow experience is over. This could be a simple case of not wishing to spare the mental bandwidth for something unrelated to the goal, but instructions for maintaining flow often include instructions to deliberately not think about mistakes one has made during the operation. That would be of questionable utility if the goal is to conserve bandwidth, but it would explain quite a lot if the important thing is to maintain a mental model with no significant internal subdivisions. 
The subjective experience of flow is simultaneously the strongest and the weakest argument for the temporal holonic theory of flow. It could be called the strongest, because the time-holon theory is a familiar description of how flow state feels. However, flow state is characterized by a reduction of activity in those parts of our brain that model time – it’s entirely plausible that our subjective experience of flow is misleading, and that any attempt to study the phenomenon by referring to our subjective experience of it is necessarily working off bad data. 
Even if our subjective memories of flow experiences are not indicative that flow states arise from temporal holonics, it may still be useful to associate the two. This is because whether or not flow state is an example of conditions in which the wave function of a process expands its range of possible states, it is nearly a given that such conditions exist. It follows that the ‘infinitely divisible’ model of time is an impediment to how we model the physics of processes, and that a holonic theory of time would be relevant somewhere, even if it has nothing to do with flow states. Our subjective experience of flow states might at least help us think intuitively about temporal holonics where they do occur. Thus we can approach the same central point from opposite directions: Our subjective experience of flow provides an intuitive description for this novel model of time. And the holonic view of time provides a speculative but consistent explanation of some of the more unusual mental states humans experience. 
An advantage of taking a mathematical, data-centric view of the world is that it allows us to describe very complex realities in very simple terms. A prime example is that of describing the world or area of interest as a Hilbert space, or high-dimensional matrix. A typical system of arbitrary complexity could be represented as a 4-D matrix with each particle having 3 spatial co-ordinates at every time co-ordinate. Of course, such a matrix might not be the most complete or compact means of describing a system. A Hilbert space is, fortunately, not limited to four or any rational number of dimensions. But for ease of visualization we can limit ourselves to four dimensions when we begin to think with this cognitive tool, though it can be applied to very different parameters if the investigator wishes. 
A concrete example may be useful here. Consider a chess game. Any given chess game of M number of moves can be represented by an 8X8XM matrix- a 3 dimensional Hilbert space describing each square on each turn. Depending on how we count duplicate pieces, each of the 64M points in the Hilbert space that describe one game would have between 15 and 33 possible values. 
Suppose we begin with a simple, 100-move game and count fungible pieces (e.g. all white pawns) only once – in this case there would be an upper bound of only 8*8*100*15=96000 possible Hilbert spaces describing the various 100-move chess games possible. This is a large number, but not impossible to imagine – we can visualize walking among 960 ‘Stacks’ of games, each comprised of 100 vertically-arranged chessboards showing one possible progression from turn 0 to turn 100.
In fact it would be fewer than 960, because the majority of theoretically possible progressions do not describe an actually possible state of play. The possible states a square can have at any given time step are sharply limited by the states of the squares around it at the previous time step. In other words, while a given square could be in any of 15 states eventually, the need for a valid progression to that state from the set value it held at turn 0 sharply reduces the number of possible games below the upper bound of 96000.
The sort of 4-dimensional Hilbert space with which we can describe an arbitrary physical system is very similar to this abstract, idealized idea of a complete chess game. Two significant differences exist however: being four-dimensional, we cannot visualize it using spatial intelligence. This presents difficulty in specific instances, but in an abstract sense it is still an intuitively satisfying way of thinking about the system in question.
The other difference is rather more significant: where the pieces of a chess game have discrete, clearly identifiable locations, particles in a 3D space are properly represented by either rational numbers or confidence intervals. With our current understanding of nonlocality, it may be more accurate to think of the ‘position’ of a particle as a normal distribution, which can be made steeper by more accurate measurement but never collapsed to a single location of confidence 1. This adds an additional level of difficulty if we are trying to be rigorous, but as an intuition pump the image of our ‘chess towers’ should still help us approach our 4 dimensional reality as a Hilbert space. 
Withing this context, a holonic model of time can be directly accessed. Consider a Hilbert space describing a chess game. Given the first and last positions with an arbitrary number of moves in between, we can deduce a small number of possible progressions the system may have undergone between state 1 and state 2.  Some transitions are more likely than others, as the moves an opponent makes are always partially predictable, and reducing our opponent’s options is an important part of any strategy. This is an excellent image to keep in mind while modelling a more abstract idea of agency. In a game of chess there are factors we control and factors we don’t, desired outcomes and multiple paths to them. A chess game is a useful metaphor in particular because being two-dimensional, we can imagine progression through time via a third dimension. If we wish to apply this sort of analysis to any system where time is important – as I argue a system involving flow is - we will need to return to our less easily-visualized 4D Hilbert space. 
A person experiencing flow is essentially taking a low-probability path through our 4D Hilbert space – a high probability of a desired outcome removes the challenge necessary for flow state to arise. At macroscopic scales probability - though important - is so complex  and relies on so many nonrandom factors that it cannot be easily imagined as an abstract, universally applicable set of dynamics.
In our hypothetical 4D Hilbert space however, probability can be directly represented. Consider a plane with a smoothly distributed probability of finding a particle at any given point:
 
.03125	.0625	.125	.0625	.03125
.0625	.125	.25	.125	.0625
.125	.25	.5	.25	.125
.0625	.125	.25	.125	.0625
.03125	.0625	.125	.0625	.03125

Setting aside for now what happens when an observation in this space is made, this kind of ‘probability density’ [22] is a standard tool for visualising particles interacting. If we replace our grid of squares with a grid of cubes, we have a probability distribution in 3D space. Replace the cubes with tesseracts, and we are visualizing probability distributions through time.
Most of the time, the path we take through such a space can be comfortably broad, the same destination being accessible via various sequences of values clustered loosely around one or more ‘ideal’ paths. When successfully navigating between a given beginning and end state, the number of possible paths, or configurations encompassing the beginning and end states narrows – possible ‘paths’ become more tightly constricted. What form this narrowing takes can depend to a significant degree on the form taken by the aimed-for end state. 
Consider now the necessary prerequisites for flow to occur: one of the most universally accepted precursors is intense mental focus in response to a challenging situation. Flow is thought to occur most reliably when a high degree skill is applied to a high degree of challenge.
To approach a mathematical description of this dynamic, we could represent a skill by the ability to navigate a particular sort of four-dimensional landscape – to connect an initial state to a desired end state through a Hilbert space via continuous intermediate states. 
Suppose then that a given skill challenge requires observing a set of intermediate states between an initial and desired condition. We might also say that what is truly required to successfully navigate such a landscape is to avoid observing any intermediate states that rule the desired end state out. If the skill challenge is easy, the number of intermediate states that can be safely observed is relatively large. 
As a skill challenge increases, the number of acceptable intermediate states decreases. The response of someone pursuing excellence in the skill in question is to narrow their focus, putting ever more energy into following the narrow path of intermediate states that connects the initial and desired conditions. 
In his book ‘The Rise of Superman’ [23] Steven Kotler catalogues the remarkable rate of advancement in extreme sports over the latter half of the twentieth century, an excellent study of these ‘narrow paths’ in a practical field. One interesting aspect of this survey is that as soon as a particular record is broken, the odds of it being broken again by someone else in the same discipline appear to increase dramatically. 
Rupert Sheldrake’s theory of Morphic Resonance [24] would predict this behaviour along with similar dynamics in many other fields. This theory posits that events occurring increase the likelihood that similar events will occur again. If we consider this dynamic in the context of our four-dimensional matrix, we might say that when two states appear adjacent at one strata of the matrix increases the likelihood that we will observe adjacent states like them at another strata.
QBism is an interpretation of Quantum Mechanics that may offer an elegant mechanism to explain dr. Sheldrake’s theory. QBism interprets much of the behaviour of quantum systems as subjective, with states such as superposition representing what an agent could reasonably predict about the system in question [25]. If a set of states have been observed in proximity at one point in the past, it stands to reason that if one observes one of them again at a future date, the odds of seeing the others in a familiar configuration go up.
It may help us conceptually to re-examine the nature of nonlocality in this context. We typically think of a particle in superposition as being observable at any number of points in space given a specific point in time. We do not think of its coherence as extending across time, but new effects become conceivable if we do. Entanglement across not only spatial but temporal distances would allow new kinds of information exchange, including the sort of Bayesian predictive models prominent in QBism. To revisit our chess metaphor, if layout L(n) is often followed by layout L(n+1) then observing either of these layouts at a given point along our ‘time’ axis helps us draw conclusions about the odds of encountering the other, much as observing a particle in space allows us to draw conclusions about a particle with which it is entangled. 
From a Bayesian perspective, Sheldrake’s theory of morphic resonance would be a natural consequence of extending quantum mechanics through the time dimension without the need for an extra field – insofar as QBism is an interpretation that focuses on how we relate to the unknown, consistently appearing next to one another in the past is an excellent indicator that discretizeable states will appear adjacent to one another in the future. Just as atoms are holons that can join together into larger holons like molecules, states are holons that can join together into larger holons like processes. 
The more unlikely a given outcome is for a given initial state, the ‘narrower’ the path between the two becomes. This may paradoxically increase the learning rate for crossing such a state-space, as successfully navigating it once would more strongly adjust the likelihood that any intermediate state which had occurred during the first successful crossing would appear adjacent to the initial state on a subsequent attempt. These temporally adjacent states need not all have been observed explicitly in the past – an observed state can have multiple possible precursor states that could all be in superposition. However, a chain of events leading to an unlikely outcome has fewer possible states comprising it, therefore knowing an unlikely outcome occurred provides more information about the chain of events leading to it than a likely outcome. This interpretation of QBism, in the context of entanglement between temporally adjacent states, could provide not just a mechanism for Sheldrake’s ‘Morphic Resonance’ but also an explanation for Kotler’s observation that learning happens more quickly when the sequence of events is less likely.
If this version of Temporal Holonics is accurate, it implies a potential method for entering flow. If we view the entire time span containing an experience of flow as a single, coherent holon, then the series of actions needed to successfully navigate the space as desired are one of many possible states that could exist between the initiation of the ‘timeless’ transition and its conclusion. The skills to potentially take all the correct actions between the beginning and end of a challenge requiring flow need to be present so that the optimal path through probability space exists as potential. To realise that potential requires an ability to enter coherence with past and future ‘selves’, to make the unlikely ‘best’ route through the landscape possible, and to observe those states where anchoring the process would be useful. Much as Mcfadden and Al-Kalili posit that useful mutations ‘register’ in recordkeeping while useless ones do not, lucky breaks and good ideas may ‘register’ during a flow state while the myriad paths that lead to a bad outcome remain as unrealized potential. Rigorous experimentation with deliberate directing of attention may reveal a method to reliably trigger such transitions. It would seem likely that if such experimentation bears fruit, much of the technique will involve deliberately not observing the multiple ways an agent could arrive at an undesired outcome. 
Motivation
Humans broadly categorise our own modes of thought along holistic or analytical lines. The current conception of time is particularly analytical in nature, and this ‘infinitely divisible’ model allows some sophisticated analysis. However, it is likely that such an approach also destroys some of the information potentially present, some of which may be valuable. 
A holonic theory of time, in which coherence is extended not just across space but also time, allows a more mixed approach. Viewing some processes as holons occupying bounded regions of time opens the door to more holistic approaches to certain problems, and prevents the loss of data that further subdividing these regions entails. By recognizing the type of data which is lost if such processes are subdivided anyway, we gain a sense of the kind of additional problem-solving capabilities to be gained by viewing some phenomena as existing across a stretch of time.  
One such capability may be an exploratory mechanism for phenomena such as prestimulus response [26]. If we view time not as an infinitely divisible backdrop wherein nothing can affect a previous state, no matter how close they are in fact, then experiments such as Spottiswoode & May’s violate our most basic assumptions. If we instead view time as an ‘uneven’ medium within which some processes cannot be subdivided without losing some information, then this class of phenomenon is perfectly in line with our assumptions – some information would exist in superposition across a span of time, and its accessibility ‘around’ a triggering event is a natural consequence of this nonlocality. 
If this assumption holds water – and numerous experiments indicate that something of this nature must be in play [27] – then a key question arises: “Can this dynamic be usefully employed, and how if so?” Much of the motivation in any study is related to practical applications. Practical applications of a holonic theory of time are a primary consideration of such an exploration, and what is more, practical applications provide much more convincing evidence for their underlying theory. 
The possible applications of a new theory are myriad, and a minor error in any theory can invalidate multiple tests based upon its reality. Conversely, if a partially correct theory has multiple potential applications, testing which of these seem promising is an excellent way to refine it, with the cost of testing being offset by the benefits of a successful test. 
Time is often considered external to the experiments in QM [28], and experiments are generally constructed on the assumption that an observation causing a wave function collapse can happen at any arbitrary point in time. Though this is largely borne out by observation, it does not address the fact that observations cannot be made with arbitrary precision, nor the possibility that some processes could resist observation. 
The ‘localized’ nature of time in Relativity is sufficiently well-established at large scales that a theory of quantum mechanics compatible with it would likely have to account for this effect. There is in fact an interpretation compatible with the localized truths and timekeeping that characterises the theory of Relativity.
Phenomena like the quantum Zeno effect are observed at very small scales – but it may be possible to apply the same principles to very large scales as well. Consider the interpretation of the quantum Zeno effect put forth earlier, which suggests that in order to move, a particle has to entangle with a state adjacent in both time and space – and that preventing entanglement by repeatedly observing the particle in one place prevents it from gaining the necessary energy to be observed anywhere but its initial state. 
Now consider the creatively named phenomenon of spaghettification. This phenomenon refers to the fact that when an object is accelerated to near light speeds, such as while approaching an event horizon, it appears to ‘stretch’ along the direction of its velocity. If we apply a similar interpretation to this phenomenon, the logic of the ‘entanglement is required for motion’ theory would predict that a very fast-moving object is entangled with so many adjacent states along its direction of motion that this ‘stretched’ set of states appears as a single holon with a ‘longer’ extent along one direction. 
We tend to think of the Copenhagen interpretation as punctuating a period of ‘wavelike’ behaviour with an instant in which collapse reveals our object of measurement in a real and perfectly localized state. But in fact, this localization should only occur to the degree that we know the timing and result of the measurement. We tend to think of the ‘wave-particle’ duality as a strict binary, with a wave pattern ‘collapsing’ into a local particle upon observation. It is probably more accurate to say that a wave pattern’s distribution ‘narrows’ to fit within the measurement error of the observation causing the collapse. We imagine wave collapse something like this:
                   
When in actual fact, it is probably more accurate to imagine it like this:
                   
With a new, narrower probability distribution that is not qualitatively different from the previous one.
Our near-lightspeed object could be seen similarly as an object that had such wide probability across one dimension that measuring its position along this direction does not collapse its position beyond a certain point – in other words, it ‘resists’ collapse.
In some sense the question of whether it is more predictive to think in terms of fields or particles could be seen as an expression of the same dynamic. Rather than see the wave-particle duality as a binary, we might instead say that the nature of the interaction determines whether we see a wider or a narrower distribution of states. A very precise measurement narrows the possible states to a very small set of possible states, while a more general measurement allows a wider range of possibilities in the system with which a given observer interacts. 
This quality of ‘resisting’ collapse is important if we are to move from theoretical to practical investigation. Time to decoherence can be calculated given the right combination of conditions and assumptions [29]. If we consider it in the macroscopic sense, it would correspond to the ability to maintain multiple potential states simultaneously. By way of contrast, the ability to enter flow state consistently seems to require not simply existing in a superposition of multiple states across a period of time, but also being able to ‘recognise’ the ‘preferred’ chain of events. 
The latter seems to primarily require domain knowledge. It’s been hypothesized that diligent practise followed by relaxation is a necessary precursor for entering flow state [30]. Anecdotally, flow seems to require visualising the next step in the process as it unfolds, which is only possible with an accurate domain knowledge. 
If a flow state is analogous to a superposition of multiple states and domain knowledge allows us to ‘observe’ a more optimal ‘path’ through the possibilities, we would assume something else determines whether enough states are ‘included’ in the first place. Given that successfully entering flow state requires a large percentage of mental resources to be dedicated to a single task, a similarly large amount of mental energy seems to be dedicated to including more potential states in the brain’s mental model of the process. If the human brain’s functionality lies in partially modeling the universe, it stands to reason that dedicating more mental resources would allow a more complex model – which runs and evaluates more scenarios in parallel – to be made. 
Paradoxically, so might the inclusion of fewer time steps. Consider a go player who seemingly wastes a move by placing a stone where it is not helpful to his strategy until very late in the game. Modelling possible progressions of a lengthy game requires ‘beginning’ at some ‘fixed’ state of the board and diverging the various models from there. Each ‘fixed’ state so envisioned requires a new model be made starting at the hypothetical state. The more skilled a player is, the farther from a desired outcome he can place his setups, while still accurately modelling the steps in between. Essentially, more skill allows a player to treat a larger block of time as a single holon, with all possible states ’internal’ to the holon available to their model. 
Another codeterminant to flow is creativity, which we might usefully conflate with the introduction of novelty. Human experience is generally that doing anything novel is more difficult than doing something familiar of similar complexity. Sheldrake’s model of the universe as forming habits rather than running on static laws [23] would offer an explanation for that behaviour. But during flow, introducing novelty becomes easy – or at least, the difficulty thereof is inseparable from the difficulty of entering flow. 
If we take both Sheldrake’s morphic resonance theory and the subjective feeling of easy novelty in flow states at face value, something peculiar must account for their seeming contradiction. An obvious subject for study would be the nature of time in both models. By positing that events tend to repeat once they happen, Sheldrake’s theory implies a particular direction to time. Quantum mechanics on the other hand does not posit a direction to time, and experiments like the Quantum Eraser show that this ‘bidirectionality’ is not purely theoretical. 
The ‘Holonic’ model of time may be compatible with both theories, by presenting time as discrete units which are internally bidirectional but may still be arranged directionally within larger units. Psychological flow in this theory represents a means of ‘lengthening’ the extent of the holons along the time dimension. We might say it increases the four-dimensional ‘interiority’ of its holons relative to the default mode of cognition, which models the environment as a larger number of ‘smaller’ and thus less complex holons – at least, smaller as measured along their time dimension. By reducing the number of holons in the model, flow allows for richer, though less predictable, interactions between the holons that make up the model. 
If we treat Sheldrake’s theory of morphic resonance as being limited by the number of steps, rather than a continuous and undifferentiated distance, between an event and points of resonance, the comparative ease of introducing novelty during a flow experience would be expected – essentially, by extending the active holons along the time dimension, the ‘distance’ between initial and subsequent instances of a particular pattern decreases. 
Such an ‘extension’ of holons necessarily extends interiority – it ensures fewer interactions with the components comprising a holon at a similar scale. As it is interactions that lead to decoherence, an agent who experiences fewer interactions with the ‘interior’ of a holon representing some action they take would lead to greater coherence internal to the agent. In that sense, entering flow could be thought of as working at a larger ‘scale’, though the ‘size’ is only extended along the time dimension. This model is quite consistent with both Oeckl’s and Penrose & Hameroff’s formulations – and importantly, quite coherent from an intuitive perspective. We might sum up as follows: ‘states’ only exist at the boundaries of interacting holons. In order for state to change – we might also say, in order for time to pass – regions of space-time must remain bounded. This makes them have no single state inside the boundary, this ‘interior’ is a linear combination of all its possible histories. To interact with the ‘interior’ of a temporal holon is to subdivide it; taken to its extreme this prevents any change at all from taking place via the quantum Zeno effect. The larger a 4D holon is, the more potential change is possible between its bounding states, and the more complex its interactions can be. 
Though consistent with established theory and intuitively satisfying, there are clearly some unanswered questions that could be asked about this model. For example, what if anything would it predict about group flow?
The boundary of an N-dimensional object has N-1 dimensions. Our default assumption when taking this approach to analyse spacetime is that we take the ‘boundary’ by omitting the time dimension, this having 3D state-space at either end of this boundary and the 4D transition amplitude in the span between. This is similar to the model already proposed. Suppose now we take the 3D ‘boundary’ at a space rather than a time dimension and apply it, not to the mathematically approachable world of single particles, but to our human-scaled one. 
At the intuitive level, this leads to some interesting images. Suppose our ‘boundary’ was the behaviour of one member of a sports team during a particular play. The ‘missing dimension’ might be his interiority – in other words, the 3D boundary could be thought of as the 2D surface of his body and/or sporting equipment with time playing the role of the third dimension. 
If our hypothetical athlete is in a state of flow, then this internal state is largely left unregistered – fewer or no ‘snapshots’ of the player’s internal state are being recorded in memory or oriented in narrative, consistent with the advice not to judge likelihood of success or failure during the process. Thus the only decoherence happens at the 3D ‘surface’ of the athlete performing the play. What might enhance or detract from his ability to work effectively with his team? 
We might call the determining factor in how well an athlete works with his team to be the amount of ‘friction’. This certainly seems like an effective metaphor for the cost of co-ordination, but insofar as it relates to the rate at which quantum coherence is lost via interaction with the environment, this term may have literal utility as well. If we describe ‘friction’ as any interaction which raises temperature, then it would stand to reason that this would be the point at which an indeterminable transition amplitude becomes a ‘fixed’ boundary state. By adding heat to the environment, any source of friction creates a lot of data about the specific interaction of surfaces which caused it, reducing the possible ’internal’ states as well. 
How then to reduce friction? It seems necessary to specify the time-boundary for this question to have meaning. To continue the sports example, an athlete needs to have a goal in mind which is shared by his teammates – achieving this goal defines the ‘state’ that the team wishes to encounter at the boundary of their transition amplitude. 
Two general approaches to reaching a desired end state with minimal friction suggest themselves. One can reduce the difficulty of each ‘step’ between the current state and the eventual target, breaking a complex plan into many individual steps that are all simple to execute. Alternatively, one can reduce the number of ‘steps’ between the current and desired state. Reducing the number of ‘steps’ or transition amplitudes means that each one is individually more complex and represents more total possibility than a simpler step. With this approach any type of friction, or ‘noise’ in the data encountered, has a greater chance of reducing the coherence, and thus derailing the plans, of the agent. However, these larger transition amplitudes also offer more possibilities to recover when such a disruption happens 
Fewer possible paths to a desired end state would naturally mean a lower probability of success, but it would not be a simple matter of more paths to success equalling a correspondingly higher likelihood of success – after all, a ‘larger’ transition amplitude also comes with more possible routes to failure. What seems a better description is that the more known routes to success exist, the higher the likelihood of successfully ‘following’ one across the transition amplitude to the desired boundary state. 
This begs the question of how a determined athlete (or similar subject matter expert in a different arena) can accomplish such successful ‘steering’. We may surmise, given the appearance of ‘focus’ in so many analyses of how to successfully navigate such situations, that the key skill may be better described as the ability to interact with the relevant measuring devices so as not to be ‘caught’ in a state incompatible with the desired end state. 
This exposes a possible contradiction in the model – if reducing the possible ‘paths’ during execution leads to desired results, then maintaining coherence across a larger holon would increase the potential routes to failure at a much faster rate than the potential routes to success. While intuitively sensible – nearer-term goals are evidently easier to achieve – it would seem counterproductive to create ‘larger’ holons in time for any reason. 
One reason our intuition in this case may not square with our experience is the quantum Zeno effect – some minimum transition amplitude seems necessary for any change to take place at all. Extrapolating from the simple example of a particle needing some time unobserved in order to move a minimum distance, we can assume that larger changes between beginning and ending states would require ‘larger’ transition amplitudes connecting the two. Retroactively the ‘history’ of the transition amplitude can be inferred – being of a higher dimension than the boundary states, a transition amplitude can contain all the information a series of states would and much more – but during transition, this information cannot always be allowed to propagate into the environment. 
From the concept of relaxation time, or the time required for a particle’s energy to dissipate into its surroundings, we might generalize that the greater a potential transition is, the more energy is required for it to take place. Extending the relaxation time of a holon is analogous to saving all this energy for a single purpose, thus allowing more extreme transitions to occur. Achieving difficult transitions of states may therefore be thought of as requiring both a clear idea of the desired end state while reducing the rate of energy dissipation during transition. The less energy is dissipated into the environment, the more impressive the changes that can take place across the holon. In macroscopic terms, what we might find of interest is strategies to minimise energy dissipation – the more energy can be diverted from its default paths for dissipation, the more would be available to power changes across the temporal holon of interest and the more significant the changes we would expect to observe.

 
 
Bibliography
[1] Wolchover, Natalie (December 1, 2016). "Quantum Gravity's Time Problem". Quanta Magazine.
[2] Koestler, Arthur (1967). “The Ghost in the Machine”. Macmillan 
[3] Sudarshan, E. C. G.; Misra, B. (1977). "The Zeno's paradox in quantum theory". Journal of Mathematical Physics. 
[4] Feynman, Richard P.; Robert B. Leighton; Matthew Sands (1965). “The Feynman Lectures on Physics, Vol. 3”. Addison-Wesley
[5] Yoon-Ho, Kim; Yu, R; Kulik, S.P.; Shish, Y.H.; Scully, Marlan (2000) “A Delayed Choice Quantum Eraser”. Physical Review Letters
[6] Oeckl, Robert (January 2004). “The General Boundary Approach to Quantum Gravity”. Proceedings of the first internation conference on physics, Amirkabir University, Tehran
[7] Penrose, Roger (1989). “Shadows of the Mind: A Search for the Missing Science of Consciousness”. Oxford University Press.
[8] Tononi, Giulio (2004) “An Information Integration Theory of Consciousness”. BMC Neuroscience
[9] Lambert, Chen, Et.Al. (2013) “Quntum Biology” Nature Physics
[10] Anderson, Mark (2007). “New Experiment Probes Weird Zone Between Quantum and Classical”. Wired Magazine
[11] Einstein, Albert (author); Lawson, Robert (translator) (1920). “Relativity: The Special and the General Theory” University of Sheffield 
[12] McFadden, John; Al-Khalili, Jim (June 1999). “A quantum mechanical model of adaptive mutation”. Biosystems
[13] Zurek, W.H. (1991). “Decoherence and the transition from quantum to classical” Phys. Today 
[14] Frauchiger, Daniela; Renner, Renato (2018). “Quantum theory cannot consistently describe the use of itself”. Nature Communications 
[15] Zielinski et al (2005). “Quantum States of Atoms and Molecules” Journal of Chemical Education
[16] O’Neill Shermer L, Rose KC, Hoffman A. (2011). “Perceptions and credibility: Understanding the nuances of eyewitness testimony.” Journal of Contemporary Criminal Justice. 
[17] Mikhailovich Bitsilli, Petr. (1983) “Chekhov’s art, a stylistic analysis”. Arids
[18] Mischel, Walter; Ebbesen, Ebbe B. (October 1970). "Attention in delay of gratification". Journal of Personality and Social Psychology. 
[19] Dietrich, Arne (December 2004). “Neurocognitive mechanisms underlying the experience of flow”. Consciousness and cognition vol 13, issue 4.
[20] Gretzky, Wayne (apocryphal)
[21] StarCraft 2 Design Team (July 2020) “Reflections on a decade: The best games of competitive Starcraft 2, Part 1: Wings of Liberty” news.blizzard.com
[22] Born, Max (December 1954) “The statistical interpretation of quantum mechanics”, Nobel Prize Lecture
[23] Kotler, Steven (March 2014), “The Rise of Superman: Decoding the Science of Ultimate Human Performance”, New Harvest
[24] Sheldrake, Rupert (September 2009), “Morphic Resonance: The Nature of Formative Causation”, Park Street Press
[25] Cabello, Adan (2017), “Interpretations of Quantum Theory: A Map of Madness”, Cambridge University Press
[26] Spottiswoode & May, “Skin Conductance Prestimulus Response: Analyses, Artifacts and a Pilot Study” Laboratories for Fundamental Research
[27] Honorton, Ferrari, and Hansen, ”Meta-analysis of Forced-Choice Precognition Experiments (1935-1987)”, Prepared for the US Army Medica Research and Development Command
[28] Olival Freire Jr., (2005) "Science and exile: David Bohm, the hot times of the Cold War, and his struggle for a new interpretation of quantum mechanics", Historical Studies on the Physical and Biological Sciences, Volume 36, Number 1
[29] Paz, Habib, and Zurek, (January 1993), “Reduction of the wave packet: Preferred observable and decoherence time scale”, Physical Review D, Volume 47
[30] Benson, Beary, & Carol (1974) “The Relaxation Response” Psychiatry: Interpersonal and Biological Processes, Volume37
[31] Hadley, Mark J, (1996) “The Logic of Quantum Mechanics Derived from Classical General Relativity” Foundations of Physics Letters 10(1) 43-60

